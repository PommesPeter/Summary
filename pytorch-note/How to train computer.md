# 深度学习入门

## 什么是机器学习？
机器学习大部分都是通过算法，基于统计学原理，使用数据来进行对自身模型的训练，最终输出一个能够解决各种问题的的模型。机器学习是计算机通过不断地模拟，处理数据来对自身模型的一个完善。

### 机器学习的分类

> 以下引用自百科:[https://baike.baidu.com/item/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/217599](https://baike.baidu.com/item/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/217599)

- 基于**学习策略**的分类
1. 模拟人脑的机器学习
符号学习：模拟人脑的宏现心理级学习过程，以认知心理学原理为基础，以符号数据为输入，以符号运算为方法，用推理过程在图或状态空间中搜索，学习的目标为概念或规则等。符号学习的典型方法有记忆学习、示例学习、演绎学习.类比学习、解释学习等。
神经网络学习(或连接学习)：模拟人脑的微观生理级学习过程，以脑和神经科学原理为基础，以人工神经网络为函数结构模型，以数值数据为输人，以数值运算为方法，用迭代过程在系数向量空间中搜索，学习的目标为函数。典型的连接学习有权值修正学习、拓扑结构学习。
2. 直接采用数学方法的机器学习
主要有统计机器学习.
统计机器学习是基于对数据的初步认识以及学习目的的分析，选择合适的数学模型，拟定超参数，并输入样本数据，依据一定的策略，运用合适的学习算法对模型进行训练，最后运用训练好的模型对数据进行分析预测。
统计机器学习三个要素：
	模型(model)：模型在未进行训练前，其可能的参数是多个甚至无穷的，故可能的模型也是多个甚至无穷的，这些模型构成的集合就是假设空间。
	策略(strategy)：即从假设空间中挑选出参数最优的模型的准则。模型的分类或预测结果与实际情况的误差(损失函数)越小，模型就越好。那么策略就是误差最小。
	算法(algorithm)：即从假设空间中挑选模型的方法(等同于求解最佳的模型参数)。机器学习的参数求解通常都会转化为最优化问题，故学习算法通常是最优化算法，例如最速梯度下降法、牛顿法以及拟牛顿法等。
- 基于**学习方法**的分类
(1) 归纳学习
符号归纳学习：典型的符号归纳学习有示例学习、决策树学习。
函数归纳学习(发现学习)：典型的函数归纳学习有神经网络学习、示例学习、发现学习、统计学习。
(2) 演绎学习
(3) 类比学习：典型的类比学习有案例(范例)学习。
(4) 分析学习：典型的分析学习有解释学习、宏操作学习。 [2] 
- 基于**学习方式**的分类
(1) 监督学习(有导师学习)：输入数据中有导师信号，以概率函数、代数函数或人工神经网络为基函数模型，采用迭代计算方法，学习结果为函数。 [2] 
(2) 无监督学习(无导师学习)：输入数据中无导师信号，采用聚类方法，学习结果为类别。典型的无导师学习有发现学习、聚类、竞争学习等。 [2] 
(3) 强化学习(增强学习)：以环境反惯(奖/惩信号)作为输人，以统计和动态规划技术为指导的一种学习方法。 [2] 
基于数据形式的分类
(1) 结构化学习：以结构化数据为输人，以数值计算或符号推演为方法。典型的结构化学习有神经网络学习、统计学习、决策树学习、规则学习。 [2] 
(2) 非结构化学习：以非结构化数据为输人，典型的非结构化学习有类比学习案例学习、解释学习、文本挖掘、图像挖掘、Web挖掘等。 [2] 
- 基于**学习目标**的分类
(1) 概念学习：学习的目标和结果为**概念**，或者说是为了获得概念的学习。典型的概念学习主要有示例学习。
(2) 规则学习：学习的目标和结果为**规则**，或者为了获得规则的学习。典型规则学习主要有决策树学习。
(3) 函数学习：学习的目标和结果为**函数**，或者说是为了获得函数的学习。典型函数学习主要有神经网络学习。 [2] 
(4) 类别学习：学习的目标和结果为**对象类**，或者说是为了获得类别的学习。典型类别学习主要有聚类分析。
(5) 贝叶斯网络学习：学习的目标和结果是**贝叶斯网络**，或者说是为了获得贝叶斯网络的一种学习。其又可分为结构学习和多数学习。 [2]

### 常用算法
- **决策树算法**
决策树及其变种是一类将输入空间分成不同的区域，每个区域有独立参数的算法。决策树算法充分利用了树形模型，根节点到一个叶子节点是一条分类的路径规则，每个叶子节点象征一个判断类别。先将样本分成不同的子集，再进行分割递推，直至每个子集得到同类型的样本，从根节点开始测试，到子树再到叶子节点，即可得出预测类别。此方法的特点是结构简单、处理数据效率较高。
- **朴素贝叶斯算法**
朴素贝叶斯算法是一种分类算法。它不是单一算法，而是一系列算法，它们都有一个共同的原则，即被分类的每个特征都与任何其他特征的值无关。朴素贝叶斯分类器认为这些“特征”中的每一个都独立地贡献概率，而不管特征之间的任何相关性。然而，特征并不总是独立的，这通常被视为朴素贝叶斯算法的缺点。简而言之，朴素贝叶斯算法允许我们使用概率给出一组特征来预测一个类。与其他常见的分类方法相比，朴素贝叶斯算法需要的训练很少。在进行预测之前必须完成的唯一工作是找到特征的个体概率分布的参数，这通常可以快速且确定地完成。这意味着即使对于高维数据点或大量数据点，朴素贝叶斯分类器也可以表现良好。 [4] 
- **支持向量机算法**
基本思想可概括如下：首先，要利用一种变换将空间高维化，当然这种变换是非线性的，然后，在新的复杂空间取最优线性分类表面[8]。由此种方式获得的分类函数在形式上类似于神经网络算法。支持向量机是统计学习领域中一个代表性算法，但它与传统方式的思维方法很不同，输入空间、提高维度从而将问题简短化，使问题归结为线性可分的经典解问题。支持向量机应用于垃圾邮件识别，人脸识别等多种分类问题。 [4] 
- **随机森林算法**
控制数据树生成的方式有多种，根据前人的经验，大多数时候更倾向选择分裂属性和剪枝，但这并不能解决所有问题，偶尔会遇到噪声或分裂属性过多的问题。基于这种情况，总结每次的结果可以得到袋外数据的估计误差，将它和测试样本的估计误差相结合可以评估组合树学习器的拟合及预测精度。此方法的优点有很多，可以产生高精度的分类器，并能够处理大量的变数，也可以平衡分类资料集之间的误差。 [4] 
- **人工神经网络算法**
人工神经网络与神经元组成的异常复杂的网络此大体相似，是个体单元互相连接而成，每个单元有数值量的输入和输出，形式可以为实数或线性组合函数。它先要以一种学习准则去学习，然后才能进行工作。当网络判断错误时，通过学习使其减少犯同样错误的可能性。此方法有很强的泛化能力和非线性映射能力，可以对信息量少的系统进行模型处理。从功能模拟角度看具有并行性，且传递信息速度极快。 [4] 
- **Boosting与Bagging算法**
Boosting是种通用的增强基础算法性能的回归分析算法。不需构造一个高精度的回归分析，只需一个粗糙的基础算法即可，再反复调整基础算法就可以得到较好的组合回归模型。它可以将弱学习算法提高为强学习算法，可以应用到其它基础回归算法，如线性回归、神经网络等，来提高精度。Bagging和前一种算法大体相似但又略有差别，主要想法是给出已知的弱学习算法和训练集，它需要经过多轮的计算，才可以得到预测函数列，最后采用投票方式对示例进行判别。 [4] 
- **关联规则算法**
关联规则是用规则去描述两个变量或多个变量之间的关系，是客观反映数据本身性质的方法。它是机器学习的一大类任务，可分为两个阶段，先从资料集中找到高频项目组，再去研究它们的关联规则。其得到的分析结果即是对变量间规律的总结。 [4] 
- **EM（期望最大化）算法**
在进行机器学习的过程中需要用到极大似然估计等参数估计方法，在有潜在变量的情况下，通常选择EM算法，不是直接对函数对象进行极大估计，而是添加一些数据进行简化计算，再进行极大化模拟。它是对本身受限制或比较难直接处理的数据的极大似然估计算法。 [4] 
- **深度学习**
深度学习(DL, Deep Learning)是机器学习(ML, Machine Learning)领域中一个新的研究方向，它被引入机器学习使其更接近于最初的目标——人工智能(AI, Artificial Intelligence)。
深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字，图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。 深度学习是一个复杂的机器学习算法，在语音和图像识别方面取得的效果，远远超过先前相关技术。
深度学习在搜索技术、数据挖掘、机器学习、机器翻译、自然语言处理、多媒体学习、语音、推荐和个性化技术，以及其他相关领域都取得了很多成果。深度学习使机器模仿视听和思考等人类的活动，解决了很多复杂的模式识别难题，使得人工智能相关技术取得了很大进步。
## 什么是计算机视觉？
通俗来讲就是电脑通过摄像头能够像人一样获取各种信息，就好比电脑的cpu相当于人类的大脑，而摄像头就是人的眼睛。人在观察事物的时候所有的视觉信息通过眼睛后面的细胞来接收，再将信息传输到大脑里，使我们能够接受到各种信息。同理，计算机视觉正是模仿人的这一原理来让计算机能够获取各种信息。

因此，我们通常都是利用深度学习方法处理计算机视觉的问题：这个过程就类似于人类通过不断观察来获取信息。同理通过计算机视觉来获取信息，再由建立的深度学习模型通过观察各种图片的特征来学习，最后输出一个理想的模型，来达到我们的目的。

## 什么是卷积神经网络？
这是一种网络架构模型。卷积神经网络仿造生物的视知觉（visual perception）机制构建，可以进行监督学习和非监督学习，其隐含层内的卷积核参数共享和层间连接的稀疏性使得卷积神经网络能够以较小的计算量对格点化（grid-like topology）特征。卷积神经网络更符合人类大脑的工作模式、工作原理。能够更好地提取输入图像中的特征。（因为是类似人的视觉处理）

## 机器是如何学习的?

第一步：
输入数据，将数据图片分为三组，训练集，验证集，测试集。训练集用来给神经网络进行训练。验证集来验证训练的正确率。测试用来最终测试检测。
 **为什么会分训练集和验证集（数据集问题）：**
>  因为可能会出现并不是我这个模型判断出来的结果，可能只是我把标签泄露给了模型。 
> 相当于就是高考，我们就是老师，模型就是学生，训练集就是平时的小测试，有效集就是平时的模拟考，测试集就是真正高考 
> 平时训练完了之后进行模拟测试，那么此时如果他只是在训练集里面正好只是背下了答案，但其实根本不会去判断，到真正去高考的时候就做不对了 
> 这样就会导致过拟合，就是没实际学习到，只是背答案而已
> 测试集是不能够泄露给模型的，简单来说测试集就是一次考试，只要知道了答案再怎么做也不是考试。 
> 验证集就是为了验证，比如：如果有几个学生经过同一个训练集训练，在你提问他一个问题的时候：一个歪歪扭扭的3，下面的同学就开始猜了，猜5，1，3，7，你就看到第三个同学对了
> 你可能就会比较喜欢这个同学，但是其实是有可能他对这个测试集正好匹配得比较好。所以这里就会产生不同，到底是因为他真的厉害，还是因为他正好猜对了
> 因为我们是能知道答案的，当我们看到一个猜对了的同学之后，就会带上个人感情，就很喜欢这个同学，但是这其实间接地把答案泄露给了同学，就会导致他做的会很符合我们的预期
> 因为我们已经知道答案了，所以，我们要排除掉个人感情，因此要排除这种可能就要引入验证集，验证集我们能够知道答案，而测试集我们和模型都不能知道答案。
> 而且不能用测试集跑太多次，跑太多次就也会变成训练集了，测试集只能跑一次 
> 还有一个例子就是股票，（设现在为2019.1.1）如果要预测股票，就要给他2017.1.1之前的数据，要预测2017之后的数据，不同的模型产生不同的因子来预测2017年之后的然后将预测的好的去跑2019.1.1之后的，还未发生的，然后在预测现实发生过的2019.1.1之后的时候准确率就会大大下降，这个就是**过拟合。

第二步：训练

之后，数据会从神经网络的输入层输入到隐藏层，再得到一些输出，转化成数学语言：找到一个函数，将图片输入到这个函数，如果这个图片是我们所想要结果就输出1，否则输出0（激活函数（信号函数））再从输出层输出结果。

当然，监督学习的输出也可以是任意值，而不仅仅是0或者1。举另一个例子，我们的神经网络可以预测一个人还信用卡的**概率**。这个概率可以是0到100的任意一个数字**。这种问题通常叫做回归。**
通过神经网络对预测的概率再对概论进行统计学分析分析得出结果。

第三步：验证

至此，第一组中的数据已经全部用完。接下来我们会用验证集验证训练得到的模型的准确率。

优化模型的许多参数（超参）需要优化，因此导致第二步和第三步通常会交叉进行。常用的超参有神经网络有多少个神经元，有多少层神经元，哪个函数用来激活一个神经元（激活函数），用多快的速度来训练网络（学习速率）等等。Quora 工程师主管的这一回复很好的解释了这些超参。

第四步：应用
完成以上三步，模型就训练好了。接下来，我们可以把模型融合到程序中。模型可以提供一个 API，例如 ParentsInPicture(photo)。当应用程序调用该 API 的时候，模型会计算得到结果，并返回给应用程序。
对数据集进行标记的成本是非常高的。因此，必须确保使用网络得到的收益比标记数据和训练模型的消耗要更高。